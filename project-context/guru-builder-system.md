I want to create a system that creates these types of LLM-driven gurus for various different games. these gurus should be a combination of context layers (like we have in this project) that capture various elements of how they respond, but the guru SYSTEM should also include “knowledge files” that aren’t loaded into the guru’s context window all the time, but are REFERENCED throughout the context layers, such that those “additional knowledge” files are only loaded into the context window when they are specifically relevant, eg: in a context layer describing how the guru should approach teaching specific strategies to the final end user (the person who will ultimately be interacting with the guru), there might be something like, “when coaching the user on strategy X, read strategyX_coaching_guide.md before determining best response”. 

ultimately, the thing I want to build is a pipeline that facilitates the following:

step 1: 
the user creates / shares an initial stab at a context / instructions file that captures the user’s initial POV on the teaching philosophy / the way the user is thinking about teaching this particular game (layer 0). the user may also take a stab at / share a first version of layer 1: foundational principles of the game, and/or they may add some “knowledge files” up front.

step 2:
the system is then fed a series of different corpuses of information / asked to go research specific things. for example: “go read this online guide about backgammon strategy X.” or “go read all of the blogposts from John McBackgammonExpert” etc. this research should leverage the built-in “deep research” capabilities of whatever foundational model we are using (openAI for MVP), and the research project should be framed through the lens of:
- 1) go consume, digest, and try to synthesize this information
- 2) compare this information to our existing guru corpus — both the context layers and the additional knowledge files, and analyze the following question: after learning these additional things I just learned, are there aspects of our current corpus (context layers and knowledge files) that we ought to reconsider and how? this would include adding to or editing existing context layers or knowledge files, but also adding entirely new context layers / knowledge files and, crucially also potentially DELETING / getting rid of some of what we previously had. none of these edits should actually be made immediately though — rather, the guru should generate a report that outlines its new learnings from the latest research project, its top 3-5 recommendations for how we ought to change our existing corpus (with justifications for each), and then all of the rest of its recommendations, however many it has, bucketed into high, medium, and low impact / conviction (ie. high impact / conviction means the guru is strongly advocating for this change. low means, “I think this would help a bit but we’d probably be okay without it.” or “I’m not fully confident that this change would actually improve the overall system.”

it would also be ideal if these recommendations were actual structured data objects as well (vs just creating a written report with recommendations) to facilitate an interface where the user can simple approve or reject each recommendation at the end of a “research run” and once the user has responded to all of them…

step 3:
… the system uses the output of that exercise (ie. the approved recommendations for how to change the guru’s corpus) to actually go and execute those changes — editing existing context or knowledge files, adding new ones, etc.

there should be a basic summary of whats been done once the updates are complete, mostly as confirmation to the user that the corpus has indeed been updated. then, if the user wants, they can run another “research run” with a different set of research instructions / a different target re: content that might enrich the guru