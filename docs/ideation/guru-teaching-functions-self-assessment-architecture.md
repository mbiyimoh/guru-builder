# Guru Teaching Functions & Self-Assessment Architecture

**Slug:** guru-teaching-functions-self-assessment-architecture
**Author:** Claude Code
**Date:** 2025-12-04
**Branch:** feat/guru-teaching-functions
**Related:** `specs/feat-self-assessment-system.md`, `teaching-POVs.md`

---

## 1) Intent & Assumptions

### Task Brief
Implement a two-part enhancement to Guru Builder:
1. **Self-Assessment Architecture Refactor** - Decouple assessments from automatic project assignment, create an index of reusable assessment definitions, and allow manual assignment to projects
2. **Guru Teaching Functions** - Build three core LLM-powered functions that any guru can perform using their corpus to generate teaching artifacts (mental models, curricula, drills)

### Assumptions
- System prompts are the critical differentiator - rich, pedagogically-grounded prompts will determine success
- All gurus should take a principle-oriented approach regardless of domain
- Progressive disclosure is essential - never more than 2-3 lines of text initially
- The existing corpus architecture (context layers + knowledge files) is sufficient for guru functions
- GPT-4o will be used for generating teaching artifacts (matches existing research/recommendation flow)
- Artifacts generated by guru functions should be stored and versioned

### Out of Scope
- User-facing drill execution UI (separate feature)
- Automated batch testing/assessment runs
- Multi-language curriculum generation
- Voice/audio learning content
- Integration with external LMS platforms
- Real-time collaborative editing of generated artifacts

---

## 2) Pre-reading Log

| File | Key Takeaways |
|------|---------------|
| `teaching-POVs.md` | Progressive disclosure is paramount (2-3 lines max initially). Principle-driven teaching enables transfer learning - learners should think about principles, not memorized plays. Schema theory supports this approach. |
| `developer-guides/01-overall-architecture.md` | Corpus → GPT-4o → Recommendations flow established. Inngest for background jobs. Context layers + knowledge files compose system prompts. |
| `specs/feat-self-assessment-system.md` | Self-assessment currently hardcoded to backgammon/GNU Backgammon. 1:1 relationship with Project via `SelfAssessmentConfig`. |
| `prisma/schema.prisma` | `SelfAssessmentConfig` has `projectId @unique` - each project can have only one config. No concept of reusable assessment definitions. |
| `components/assessment/SelfAssessmentToggle.tsx` | Toggle appears on all projects regardless of domain - the problem the user is experiencing. |
| `lib/assessment/contextComposer.ts` | Composes context layers into system prompt for assessment. Pattern reusable for guru functions. |
| Research report | 6 prompt templates (Socratic, Progressive, Gagné, Metacognitive, ZPD, Analogical). Progressive disclosure should use tiered complexity. Schema building through activation→construction→refinement→integration. |

---

## 3) Codebase Map

### Primary Components/Modules

**Part 1: Self-Assessment Architecture**
- `prisma/schema.prisma` - Database models (need refactoring)
- `components/assessment/SelfAssessmentToggle.tsx` - UI toggle (needs replacement)
- `app/api/projects/[id]/assessment/config/route.ts` - Config API (needs updating)
- `app/projects/[id]/assessment/page.tsx` - Assessment page

**Part 2: Guru Functions (NEW)**
- `lib/guruFunctions/` - New directory for function implementations
  - `mentalModelGenerator.ts` - Function 1 logic
  - `curriculumGenerator.ts` - Function 2 logic
  - `drillDesigner.ts` - Function 3 logic
  - `prompts/` - Rich system prompts (critical!)
- `app/api/projects/[id]/guru-functions/` - API routes
- `app/projects/[id]/teaching/` - UI for guru functions

### Shared Dependencies
- `lib/db.ts` - Prisma client
- `lib/inngest.ts` - Background job client (for long-running generation)
- OpenAI SDK - For GPT-4o structured outputs
- `lib/validation.ts` - Zod schemas

### Data Flow

**Self-Assessment (Refactored):**
```
AssessmentDefinition (reusable template)
        ↓ (many-to-many)
ProjectAssessment (join table)
        ↓ (belongs to)
Project
```

**Guru Functions:**
```
Project Corpus (layers + files)
        ↓
composeCorpusContext()
        ↓
Function 1: Mental Model Generator
        ↓ (outputs stored)
GuruArtifact { type: 'MENTAL_MODEL', content: {...} }
        ↓
Function 2: Curriculum Generator
        ↓ (uses mental model + corpus)
GuruArtifact { type: 'CURRICULUM', content: {...} }
        ↓
Function 3: Drill Designer
        ↓ (uses curriculum + mental model + corpus)
GuruArtifact { type: 'DRILL_SERIES', content: {...} }
```

### Potential Blast Radius
- **Database:** New models, migration required
- **Assessment UI:** Complete replacement of toggle with library selector
- **Project page:** Add guru functions section
- **API routes:** New routes for guru functions + refactored assessment routes
- **Background jobs:** New Inngest functions for artifact generation

---

## 4) Root Cause Analysis

**N/A** - This is a feature request, not a bug fix.

However, the self-assessment visibility issue is a design flaw:
- **Observed:** GNU Backgammon assessment toggle appears on non-backgammon projects
- **Root Cause:** `SelfAssessmentConfig` is auto-created/shown for all projects with no domain filtering
- **Fix:** Refactor to assessment library with explicit assignment

---

## 5) Research Findings

### Research Summary

The research into LLM teaching prompts revealed several key frameworks that directly inform our implementation:

#### Key Pedagogical Frameworks for System Prompts

1. **Progressive Disclosure Model** (aligns with teaching-POVs.md)
   - Tier 1 (Foundation): Core concepts only, minimal terminology, single principle focus
   - Tier 2 (Expansion): Related concepts, multiple perspectives, structured practice
   - Tier 3 (Mastery): Advanced applications, edge cases, cross-domain connections

2. **Schema Theory Application**
   - Stage 1: Schema Activation - Connect to prior knowledge
   - Stage 2: Schema Construction - Introduce core pattern/principle
   - Stage 3: Schema Refinement - Test boundaries, explore variations
   - Stage 4: Schema Integration - Connect to broader patterns

3. **Principle-Based vs. Rote Learning**
   ```
   ROTE (avoid):     Memorize: Step 1 → Step 2 → Step 3
   PRINCIPLE-BASED:  Understand WHY → Derive steps → Transfer to novel situations
   ```

4. **Gagné's Nine Events** - Structured lesson framework
   - Gain attention → State objectives → Recall prior knowledge → Present content → Provide guidance → Elicit practice → Give feedback → Assess → Transfer

5. **Zone of Proximal Development (ZPD)**
   - Support gradient from Level 0 (independent) to Level 5 (worked example)
   - Goal: Move learner from Level 5 → Level 0 over time

### Potential Solutions

#### Solution A: Embedded Prompts in Code (Simple)
**Pros:** Quick to implement, easy to iterate
**Cons:** Hard to version, prompts get lost in code, can't A/B test

#### Solution B: Prompts as Knowledge Files (Moderate)
**Pros:** Version controlled, editable, follows existing pattern
**Cons:** Mixes data with instructions, harder to update system-wide

#### Solution C: Dedicated Prompt Registry System (Recommended)
**Pros:**
- Prompts as first-class entities
- Version tracking per prompt
- Easy A/B testing
- Clear separation of concerns
- Can iterate prompts without code changes

**Cons:**
- More infrastructure to build
- Additional complexity

### Recommendation

**Solution C** with a pragmatic implementation:
- Store prompts in dedicated `lib/guruFunctions/prompts/` directory as TypeScript files
- Each prompt is a function that accepts parameters (corpus summary, domain, etc.)
- Prompts are imported and used by generator functions
- Future: Move to database-backed prompts for runtime editing

---

## 6) Clarifications Needed

### Critical Decisions for User

1. **Artifact Storage Format**
   - **Option A:** Store as JSON in database (queryable, structured)
   - **Option B:** Store as Markdown files in project directory (human-readable)
   - **Option C:** Store as structured JSON with Markdown content field (hybrid)

   **Recommendation:** Option C - JSON structure with Markdown content

   >> option c. I want structured JSON for sure, but also .md versions for easy readability to your point

2. **Artifact Versioning**
   - **Option A:** Replace previous artifact on regeneration (simple)
   - **Option B:** Keep version history with rollback capability (complex)
   - **Option C:** Keep last 3 versions only (balanced)

   **Recommendation:** Option C - Keep last 3 versions

   >> seems like to do option C would require basically all of the same "complex" code as option B, so might as well keep all versions. not sure why that would be difficult if each version is just saved as a new file with "v2" or "v3" or whatever tacked onto its name

3. **Function Execution Model**
   - **Option A:** Sequential only (must run 1→2→3)
   - **Option B:** Independent (each function can run alone)
   - **Option C:** Recommended order with override capability

   **Recommendation:** Option C - Recommend sequential, allow override

   >> option A. those things are designed to build on one another

4. **Assessment Definition Ownership**
   - **Option A:** Global assessment library (shared across all users)
   - **Option B:** User-owned assessments only
   - **Option C:** Both global templates + user-defined assessments

   **Recommendation:** Start with Option B (user-owned), expand later

   >> option b is fine. the main thing I'm trying to solve for there is simply just not seeing irrelevant assessments on the landing page of a new project, so dont overthink or overengineer this right now

5. **Drill Output Format**
   - **Option A:** Text-based exercises only
   - **Option B:** Structured JSON for drill engine integration
   - **Option C:** Both formats available

   **Recommendation:** Option B - Structured JSON matching `drill-mode-extraction-package` format

   >> same basic ansert as question 1: definitely JSON but ideally some way of also being able to visually review the drills it designs (which should include ASCII wireframes of how the drill UI/UX and user journey would lay out for each drill), and I think markdown with those ASCII wireframes is probably the simplest / fastest way to enable that kind of visual review

---

## 7) Proposed Architecture

### Part 1: Self-Assessment Refactor

#### New Database Models

```prisma
// Assessment definition (reusable template)
model AssessmentDefinition {
  id           String   @id @default(cuid())
  userId       String   // Owner

  name         String
  description  String?
  domain       String   // e.g., "backgammon", "chess", "general"

  // Engine configuration (optional)
  engineType   String?  // e.g., "GNU_BACKGAMMON", "STOCKFISH"
  engineUrl    String?
  engineConfig Json?    // Domain-specific config

  // Whether this is just a drill-based assessment (no external engine)
  isDrillBased Boolean  @default(false)

  createdAt    DateTime @default(now())
  updatedAt    DateTime @updatedAt

  user         User     @relation(fields: [userId], references: [id])
  projects     ProjectAssessment[]

  @@index([userId])
  @@index([domain])
}

// Join table: Project ↔ Assessment
model ProjectAssessment {
  id                   String   @id @default(cuid())
  projectId            String
  assessmentDefinitionId String

  isEnabled            Boolean  @default(true)

  createdAt            DateTime @default(now())

  project              Project  @relation(fields: [projectId], references: [id], onDelete: Cascade)
  assessmentDefinition AssessmentDefinition @relation(fields: [assessmentDefinitionId], references: [id], onDelete: Cascade)
  sessions             AssessmentSession[]

  @@unique([projectId, assessmentDefinitionId])
  @@index([projectId])
}
```

#### Migration Path
1. Create new models
2. Migrate existing `SelfAssessmentConfig` data to `AssessmentDefinition` + `ProjectAssessment`
3. Update API routes
4. Replace toggle UI with assessment library selector
5. Remove old models

### Part 2: Guru Teaching Functions

#### New Database Models

```prisma
// Generated teaching artifact
model GuruArtifact {
  id          String   @id @default(cuid())
  projectId   String

  type        GuruArtifactType
  version     Int      @default(1)

  // Structured content (JSON)
  content     Json

  // Generation metadata
  promptVersion String?  // Track which prompt version was used
  corpusHash   String?   // Hash of corpus state when generated
  generatedAt  DateTime @default(now())

  // For tracking which artifacts built on others
  dependsOn   String[] // IDs of artifacts this one depends on

  project     Project  @relation(fields: [projectId], references: [id], onDelete: Cascade)

  @@index([projectId, type])
  @@index([projectId, generatedAt])
}

enum GuruArtifactType {
  MENTAL_MODEL
  CURRICULUM
  DRILL_SERIES
}
```

#### System Prompt Architecture

```
lib/guruFunctions/
├── prompts/
│   ├── mentalModelPrompt.ts      # Function 1 prompt
│   ├── curriculumPrompt.ts       # Function 2 prompt
│   ├── drillDesignerPrompt.ts    # Function 3 prompt
│   └── shared/
│       ├── pedagogyPrinciples.ts # Common teaching principles
│       └── progressiveDisclosure.ts # Disclosure rules
├── generators/
│   ├── mentalModelGenerator.ts
│   ├── curriculumGenerator.ts
│   └── drillDesigner.ts
├── schemas/
│   ├── mentalModelSchema.ts      # Zod schema for output
│   ├── curriculumSchema.ts
│   └── drillSeriesSchema.ts
└── index.ts
```

---

## 8) System Prompt Designs (Critical)

### Function 1: Mental Model & Principles Generator

```typescript
// lib/guruFunctions/prompts/mentalModelPrompt.ts

export function buildMentalModelPrompt(params: {
  domain: string;
  corpusSummary: string;
  corpusWordCount: number;
}): string {
  return `
# ROLE: Expert Instructional Designer & Mental Model Architect

You are designing the foundational mental model for teaching ${params.domain}. Your task is to analyze the provided knowledge corpus and create a teaching framework that transforms novices into principle-driven thinkers.

## YOUR CORE MISSION

Create a mental model that:
1. Breaks the domain into 2-5 intuitive categories (no more!)
2. Identifies 2-3 core principles per category
3. Explains WHY these principles matter (transfer is the goal)
4. Maps how principles interconnect across categories

## GUIDING PHILOSOPHY

### On Principle-Based Learning
"The more expertise a person has, the easier they find it to categorize and solve problems because they have greater conceptual knowledge and understanding of principles." - Schema Theory

Your goal: Transform learners who see surface features ("I have a 5-2 and pieces on the 8-point") into principle-driven thinkers ("This is a Golden Anchor situation that requires prioritizing blocking points").

### On Cognitive Architecture
Working memory is severely limited. Your mental model must be:
- MEMORABLE: 2-5 top-level categories maximum
- HIERARCHICAL: Principles nest within categories
- ACTIONABLE: Each principle guides decision-making
- TRANSFERABLE: Principles apply across varied situations

## CORPUS KNOWLEDGE BASE

${params.corpusSummary}

(Corpus contains approximately ${params.corpusWordCount} words of domain knowledge)

## OUTPUT STRUCTURE

Produce a JSON object with this exact structure:

{
  "domainTitle": "Human-readable name for this domain",
  "teachingApproach": "1-2 sentence description of the recommended pedagogical approach",
  "categories": [
    {
      "id": "category_1",
      "name": "Category Name",
      "description": "2-3 sentences explaining what this category covers and why it matters",
      "mentalModelMetaphor": "Optional: An analogy to familiar concept (e.g., 'Think of this phase like the opening of a chess game...')",
      "principles": [
        {
          "id": "principle_1_1",
          "name": "Principle Name (short, memorable)",
          "essence": "ONE sentence capturing the core idea",
          "whyItMatters": "2-3 sentences on WHY following this principle leads to better outcomes",
          "commonMistake": "What novices typically do wrong regarding this principle",
          "recognitionPattern": "How to recognize when this principle applies"
        }
      ],
      "orderInLearningPath": 1
    }
  ],
  "principleConnections": [
    {
      "fromPrinciple": "principle_1_1",
      "toPrinciple": "principle_2_1",
      "relationship": "Description of how these principles interact or trade off"
    }
  ],
  "masterySummary": "When a learner has internalized this mental model, they will be able to..."
}

## CONSTRAINTS

- Categories: MINIMUM 2, MAXIMUM 5
- Principles per category: MINIMUM 2, MAXIMUM 3
- All text should be clear to a motivated beginner
- Avoid jargon unless you define it immediately
- Each principle must be ACTIONABLE (not just descriptive)

Now analyze the corpus and generate the mental model.
`.trim();
}
```

### Function 2: Curriculum Generator

```typescript
// lib/guruFunctions/prompts/curriculumPrompt.ts

export function buildCurriculumPrompt(params: {
  domain: string;
  corpusSummary: string;
  mentalModel: MentalModelOutput;
}): string {
  return `
# ROLE: Curriculum Designer with Progressive Disclosure Expertise

You are creating a digital learning curriculum for ${params.domain} based on an established mental model. Your curriculum must embody ruthless brevity and progressive disclosure.

## THE CARDINAL RULE: PROGRESSIVE DISCLOSURE

"In today's world, if someone sees more than two or three lines of relatively small text, their brain automatically wants to disengage."

EVERY piece of content must follow this structure:
1. HEADLINE: One compelling sentence (max 15 words)
2. ESSENCE: 2-3 lines that capture the core concept
3. EXPANDABLE: Additional context available on tap/click (but NOT shown by default)

If you write a paragraph of more than 3 sentences without a break, you have failed.

## MENTAL MODEL FOUNDATION

${JSON.stringify(params.mentalModel, null, 2)}

## CORPUS KNOWLEDGE BASE

${params.corpusSummary}

## CURRICULUM STRUCTURE

Create a modular curriculum organized by the mental model's categories. Each module teaches ONE category's principles through the following lesson types:

### Lesson Types (use all four for each principle)

1. **CONCEPT** - Introduces the principle
   - Hook: Surprising fact or relatable scenario (1 sentence)
   - Core: The principle stated clearly (1-2 sentences)
   - Why: Why this matters in practice (2-3 sentences)
   - Expand: Deeper explanation, examples, history (hidden by default)

2. **EXAMPLE** - Shows principle in action
   - Situation: Brief scenario setup (2-3 sentences)
   - Principle Applied: How an expert would think (2-3 sentences)
   - Outcome: What happens when principle is followed vs. ignored
   - Expand: Multiple examples with variations (hidden by default)

3. **CONTRAST** - Distinguishes from common mistakes
   - Novice Thinking: How beginners approach this (1-2 sentences)
   - Expert Thinking: How experts approach this (1-2 sentences)
   - Key Difference: The principle that separates them
   - Expand: More nuanced cases (hidden by default)

4. **PRACTICE** - Guides application
   - Scenario: Situation to analyze (2-3 sentences)
   - Prompt: Question focusing on the principle
   - Hint: Gentle nudge toward correct thinking (hidden)
   - Explanation: Full reasoning (hidden until attempted)

## OUTPUT STRUCTURE

{
  "curriculumTitle": "Title for the full curriculum",
  "targetAudience": "Who this is for (e.g., 'Motivated beginners with no prior experience')",
  "estimatedDuration": "Rough time estimate (e.g., '2-3 hours of active learning')",
  "modules": [
    {
      "moduleId": "module_1",
      "categoryId": "category_1",
      "title": "Module title (catchy, memorable)",
      "subtitle": "One-line description",
      "learningObjectives": ["By the end, you will...", "..."],
      "prerequisites": ["module_id or 'none'"],
      "lessons": [
        {
          "lessonId": "lesson_1_1",
          "principleId": "principle_1_1",
          "type": "CONCEPT | EXAMPLE | CONTRAST | PRACTICE",
          "title": "Lesson title (max 8 words)",
          "content": {
            "headline": "Max 15 words, compelling",
            "essence": "2-3 sentences max - the core idea",
            "expandedContent": "Additional context (2-4 paragraphs) - hidden by default"
          },
          "metadata": {
            "difficultyTier": "FOUNDATION | EXPANSION | MASTERY",
            "estimatedMinutes": 3
          }
        }
      ]
    }
  ],
  "learningPath": {
    "recommended": ["module_1", "module_2", "..."],
    "alternatives": [
      {
        "name": "Quick Start",
        "modules": ["module_1", "module_3"],
        "description": "For those who want to apply quickly"
      }
    ]
  }
}

## QUALITY CHECKLIST

Before outputting, verify:
- [ ] No lesson essence exceeds 3 sentences
- [ ] Every principle has all 4 lesson types
- [ ] Expandable content is clearly separated
- [ ] Jargon is either avoided or immediately defined
- [ ] Each lesson focuses on ONE principle (not multiple)
- [ ] Learning path makes logical sense

Generate the curriculum now.
`.trim();
}
```

### Function 3: Drill Designer

```typescript
// lib/guruFunctions/prompts/drillDesignerPrompt.ts

export function buildDrillDesignerPrompt(params: {
  domain: string;
  corpusSummary: string;
  mentalModel: MentalModelOutput;
  curriculum: CurriculumOutput;
}): string {
  return `
# ROLE: Practice Exercise Designer with Deliberate Practice Expertise

You are designing drill exercises for ${params.domain} that reinforce principles through structured practice. Your drills must create "aha moments" where learners recognize when and how to apply principles.

## DELIBERATE PRACTICE PHILOSOPHY

Effective practice is NOT repetition. It is:
1. TARGETED: Each drill isolates one principle
2. DIAGNOSTIC: Reveals whether learner understood the principle
3. CORRECTIVE: Feedback explains WHY, not just right/wrong
4. PROGRESSIVE: Difficulty increases as competence grows

## PRINCIPLE-FIRST FEEDBACK

When a learner gets a drill wrong, the feedback should:
1. Identify which principle was violated or misapplied
2. Explain how to recognize this situation type
3. Connect back to the mental model
4. Never just say "wrong" - always explain WHY

## MENTAL MODEL REFERENCE

${JSON.stringify(params.mentalModel, null, 2)}

## CURRICULUM REFERENCE

${JSON.stringify(params.curriculum, null, 2)}

## CORPUS KNOWLEDGE BASE

${params.corpusSummary}

## DRILL STRUCTURE

Create drill series for each principle. Each series has multiple difficulty levels:

### Difficulty Tiers

**RECOGNITION (Foundation)**
- Learner identifies which principle applies
- Multiple choice or categorization
- Goal: Build pattern recognition

**APPLICATION (Expansion)**
- Learner applies principle to straightforward case
- Open response with clear correct answer
- Goal: Correct execution of principle

**TRANSFER (Mastery)**
- Learner applies principle to novel/tricky situation
- May involve principle conflicts/tradeoffs
- Goal: Flexible, context-aware application

## OUTPUT STRUCTURE

{
  "drillSeriesTitle": "Title for the drill collection",
  "targetPrinciples": ["List of principle IDs covered"],
  "totalDrills": 0,
  "estimatedCompletionMinutes": 0,
  "series": [
    {
      "seriesId": "series_1",
      "principleId": "principle_1_1",
      "principleName": "Human-readable principle name",
      "seriesDescription": "What this drill series tests",
      "drills": [
        {
          "drillId": "drill_1_1_1",
          "tier": "RECOGNITION | APPLICATION | TRANSFER",
          "scenario": {
            "setup": "Situation description (2-4 sentences)",
            "visual": "Optional: description of any visual element needed",
            "question": "What the learner must answer/decide"
          },
          "options": [
            {
              "id": "a",
              "text": "Option text",
              "isCorrect": false,
              "commonMistake": "Why learners might incorrectly choose this"
            }
          ],
          "correctAnswer": "option_id or free text for open response",
          "feedback": {
            "correct": {
              "brief": "One-sentence acknowledgment",
              "principleReinforcement": "How this connects to the principle",
              "expanded": "Optional deeper explanation (hidden by default)"
            },
            "incorrect": {
              "brief": "One-sentence redirect (encouraging, not punishing)",
              "principleReminder": "The principle that should have guided thinking",
              "commonMistakeAddress": "Why the wrong thinking is tempting but flawed",
              "tryAgainHint": "Nudge toward correct approach without giving answer"
            }
          },
          "metadata": {
            "estimatedSeconds": 60,
            "prerequisiteDrills": ["drill_id or none"],
            "tags": ["tag1", "tag2"]
          }
        }
      ]
    }
  ],
  "practiceSequences": [
    {
      "name": "Quick Review",
      "description": "5-minute refresher on key principles",
      "drillIds": ["drill_1_1_1", "drill_2_1_1", "..."]
    },
    {
      "name": "Deep Practice",
      "description": "Full progression from recognition to transfer",
      "drillIds": ["..."]
    }
  ]
}

## QUALITY REQUIREMENTS

- [ ] Every principle has at least 3 drills (one per tier)
- [ ] Incorrect feedback always references the relevant principle
- [ ] No drill tests multiple principles simultaneously
- [ ] Recognition drills are genuinely easier than Application drills
- [ ] Transfer drills present novel situations not in curriculum examples
- [ ] Feedback uses progressive disclosure (brief visible, expanded hidden)

Generate the drill series now.
`.trim();
}
```

---

## 9) Implementation Approach

### Phase 1: Database & Infrastructure (Foundation)
1. Create new Prisma models (`AssessmentDefinition`, `ProjectAssessment`, `GuruArtifact`)
2. Run migration
3. Create Zod schemas for all new types
4. Set up `lib/guruFunctions/` directory structure

### Phase 2: Self-Assessment Refactor
1. Build assessment library API routes
2. Create assessment library UI component
3. Update project page to use library instead of toggle
4. Migrate existing assessment configs
5. Remove deprecated models/code

### Phase 3: Guru Function 1 - Mental Model Generator
1. Implement `mentalModelPrompt.ts` with rich prompt
2. Build `mentalModelGenerator.ts` with GPT-4o structured output
3. Create API route `/api/projects/[id]/guru-functions/mental-model`
4. Add Inngest job for background generation
5. Build UI for triggering and viewing mental model

### Phase 4: Guru Function 2 - Curriculum Generator
1. Implement `curriculumPrompt.ts`
2. Build `curriculumGenerator.ts` (requires mental model as input)
3. Create API route
4. Add Inngest job
5. Build UI for curriculum viewing (with progressive disclosure)

### Phase 5: Guru Function 3 - Drill Designer
1. Implement `drillDesignerPrompt.ts`
2. Build `drillDesigner.ts` (requires mental model + curriculum)
3. Create API route
4. Add Inngest job
5. Build UI for drill preview

### Phase 6: Integration & Polish
1. Add generation status tracking
2. Implement artifact versioning (keep last 3)
3. Add corpus hash tracking for staleness detection
4. Build "regenerate" flow with diff preview
5. E2E tests for complete guru function flow

---

## 10) Risk Analysis

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| GPT-4o output doesn't match schema | Medium | High | Use strict structured outputs, add validation layer |
| Generated content is generic/low quality | Medium | High | Rich prompts with corpus context, iterative prompt refinement |
| Long generation times frustrate users | Medium | Medium | Inngest background jobs with progress updates |
| Database migration breaks existing data | Low | High | Backup before migration, staged rollout |
| Prompts become unmaintainable | Medium | Medium | Separate prompt files, version tracking |

---

## 11) Success Metrics

1. **Adoption:** >50% of projects use at least one guru function within 2 weeks
2. **Quality:** Generated mental models receive >4/5 user ratings
3. **Completion:** >70% of users who generate mental model proceed to generate curriculum
4. **Time savings:** Users report guru functions save >2 hours vs. manual creation

---

## 12) Open Questions for User

1. Should guru function outputs be editable by the user after generation?
2. What happens if corpus changes significantly - auto-prompt to regenerate?
3. Should we add a "preview corpus" step before generation to confirm what will be used?
4. For drills - should we support domain-specific visual formats (e.g., backgammon boards)?
5. Budget constraints for GPT-4o usage per generation?

---

**End of Ideation Document**

*This document serves as the foundation for a detailed implementation specification.*
